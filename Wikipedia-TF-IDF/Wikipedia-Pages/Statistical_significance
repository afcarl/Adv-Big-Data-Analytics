In statistical hypothesis testing,[1][2] statistical significance (or a statistically significant result) is attained when a p-value is less than the significance level (denoted , alpha).[3][4][5][6][7][8][9] The p-value is the probability of obtaining at least as extreme results given that the null hypothesis is true whereas the significance level  is the probability of rejecting the null hypothesis given that it is true.[10] Equivalently, when the null hypothesis specifies the value of a parameter, the data are said to be statistically significant at given confidence level  = 1   when the computed confidence interval for that parameter fails to contain the value specified by the null hypothesis.[11][12]

As a matter of good scientific practice, a significance level is chosen before data collection and is often set to 0.05 (5%).[13] Other significance levels (e.g., 0.01) may be used, depending on the field of study.[14] In any experiment or observation that involves drawing a sample from a population, there is always the possibility that an observed effect would have occurred due to sampling error alone.[15][16] But if the p-value is less than the significance level (e.g., p < 0.05), then an investigator may conclude that the observed effect actually reflects the characteristics of the population rather than just sampling error.[1] Investigators may then report that the result attains statistical significance, thereby rejecting the null hypothesis.[17]

The present-day concept of statistical significance originated with Ronald Fisher when he developed statistical hypothesis testing based on p-values in the early 20th century.[18][19][20] It was Jerzy Neyman and Egon Pearson who later recommended that the significance level be set ahead of time, prior to any data collection.[21][22]

The term significance does not imply importance and the term statistical significance is not the same as research, theoretical, or practical significance.[1][2][23] For example, the term clinical significance refers to the practical importance of a treatment effect.

The concept of statistical significance was originated by Ronald Fisher when he developed statistical hypothesis testing, which he described as "tests of significance", in his 1925 publication Statistical Methods for Research Workers.[18][20][not specific enough to verify][19] Fisher suggested a probability of one in twenty (0.05) as a convenient cutoff level to reject the null hypothesis.[21] In their 1933 paper, Jerzy Neyman and Egon Pearson recommended that the significance level (e.g. 0.05), which they called , be set ahead of time, prior to any data collection.[21][22]

Despite his initial suggestion of 0.05 as a significance level, Fisher did not intend this cutoff value to be fixed, and in his 1956 publication Statistical methods and scientific inference he recommended that significant levels be set according to specific circumstances.[21]

Statistical significance plays a pivotal role in statistical hypothesis testing, where it is used to determine whether a null hypothesis should be rejected or retained. A null hypothesis is the general or default statement that nothing happened or changed.[24] For a null hypothesis to be rejected as false, the result has to be identified as being statistically significant, i.e. unlikely to have occurred due to sampling error alone.

To determine whether a result is statistically significant, a researcher would have to calculate a p-value, which is the probability of observing an effect given that the null hypothesis is true.[9] The null hypothesis is rejected if the p-value is less than the significance or  level. The  level is the probability of rejecting the null hypothesis given that it is true (type I error) and is most often set at 0.05 (5%). If the  level is 0.05, then the conditional probability of a type I error, given that the null hypothesis is true, is 5%.[25] Then a statistically significant result is one in which the observed p-value is less than 5%, which is formally written as p < 0.05.[26]

If the  level is set at 0.05, it means that the rejection region comprises 5% of the sampling distribution.[27] These 5% can be allocated to one side of the sampling distribution, as in a one-tailed test, or partitioned to both sides of the distribution as in a two-tailed test, with each tail (or rejection region) containing 2.5% of the distribution. One-tailed tests are more powerful than two-tailed tests, as a null hypothesis can be rejected with a less extreme result. Nevertheless, the use of a one-tailed test is dependent on whether the research question specifies a direction such as whether a group of objects is heavier or the performance of students on an assessment is better.[28]

In specific fields such as particle physics and manufacturing, statistical significance is often expressed in multiples of the standard deviation or sigma () of a normal distribution, with significance thresholds set at a much stricter level (e.g. 5).[29][30] For instance, the certainty of the Higgs boson particle's existence was based on the 5 criterion, which corresponds to a p-value of about 1 in 3.5 million.[30][31]

In other fields of scientific research such as genome-wide association studies significance levels as low as 69925000000000000005108 are not uncommon.[32][33]

Researchers focusing solely on whether their results are statistically significant might report findings that are not substantive[34] and not replicable.[35] To gauge the research significance of their result, researchers are therefore encouraged to always report an effect size along with p-values. An effect size measure quantifies the strength of an effect, such as the distance between two means in units of standard deviation (cf. Cohen's d), the correlation between two variables or its square, and other measures.[36]